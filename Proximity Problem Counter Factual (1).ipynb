{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56aa4590-dc9c-4f93-8e19-ad081760fea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd3a546-45b6-49ec-97ab-922bbc63645f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STARTING_LAT= 35.780175\n",
    "STARTING_LON= -78.633199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c33788-00f1-4aff-a253-09031a30d108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_table_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/datasets/default/data_for_tech_opt/customer_table.csv\")\n",
    "\n",
    "tech_table_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/datasets/default/data_for_tech_opt/tech_table.csv\")\n",
    "\n",
    "customer_schedule_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/datasets/default/data_for_tech_opt/customer_schedule.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b430a77d-e75f-4948-a511-2001bf264aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    " \n",
    "tech_table_pd = tech_table_df.toPandas()\n",
    "customer_table_pd = customer_table_df.toPandas()\n",
    "customer_schedule_pd = customer_schedule_df.toPandas()\n",
    "\n",
    "# Get unique technicians\n",
    "technicians = tech_table_pd['Technician'].unique() \n",
    "\n",
    "# Function to assign technicians to jobs\n",
    "def assign_technicians(schedule_df, technicians):\n",
    "    \"\"\"\n",
    "    Randomly assign technicians to jobs with constraints:\n",
    "    - Each tech gets 4 jobs per day (2 morning, 2 afternoon)\n",
    "    - Each tech gets 2 jobs per time slot\n",
    "    \"\"\"\n",
    "    # Create a copy to work with\n",
    "    result_df = schedule_df.copy()\n",
    "    result_df['Assigned_Tech'] = None\n",
    "    \n",
    "    # Get unique days\n",
    "    days = result_df['Day_of_Week'].unique()\n",
    "    \n",
    "    # Track assignments per tech per day and time slot\n",
    "    # Format: {day: {time_slot: {tech: count}}}\n",
    "    assignments = {}\n",
    "    \n",
    "    for day in days:\n",
    "        assignments[day] = {\n",
    "            'Morning': {tech: 0 for tech in technicians},\n",
    "            'Afternoon': {tech: 0 for tech in technicians}\n",
    "        }\n",
    "    \n",
    "    # Process each day separately\n",
    "    for day in days:\n",
    "        day_jobs = result_df[result_df['Day_of_Week'] == day].copy()\n",
    "        \n",
    "        # Process each time slot\n",
    "        for time_slot in ['Morning', 'Afternoon']:\n",
    "            slot_jobs = day_jobs[day_jobs['Arrival_Time_Slot'] == time_slot]\n",
    "            slot_indices = slot_jobs.index.tolist()\n",
    "            \n",
    "            # Shuffle the jobs randomly\n",
    "            np.random.shuffle(slot_indices)\n",
    "            \n",
    "            # Assign jobs in a round-robin fashion to ensure even distribution\n",
    "            tech_idx = 0\n",
    "            for job_idx in slot_indices:\n",
    "                # Find a tech that hasn't reached their limit for this time slot\n",
    "                attempts = 0\n",
    "                while attempts < len(technicians):\n",
    "                    tech = technicians[tech_idx % len(technicians)]\n",
    "                    \n",
    "                    # Check if this tech can take another job in this time slot\n",
    "                    if assignments[day][time_slot][tech] < 2:\n",
    "                        result_df.loc[job_idx, 'Assigned_Tech'] = tech\n",
    "                        assignments[day][time_slot][tech] += 1\n",
    "                        tech_idx += 1\n",
    "                        break\n",
    "                    \n",
    "                    tech_idx += 1\n",
    "                    attempts += 1\n",
    "                \n",
    "                if attempts >= len(technicians):\n",
    "                    print(f\"Warning: Could not assign job {job_idx} on {day} {time_slot}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49cb067-7c60-4ba5-9343-426609c2af07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility (remove this line for truly random assignments)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assign technicians\n",
    "assigned_schedule = assign_technicians(customer_schedule_pd, technicians)  \n",
    "\n",
    "assigned_schedule['job_number'] = assigned_schedule.sort_values('Job_Type') \\\n",
    "             .groupby(['Assigned_Tech','Day_of_Week','Arrival_Time_Slot']) \\\n",
    "             .cumcount() + 1\n",
    "\n",
    "assigned_schedule['Arrival_Time_Slot_int'] = np.where(assigned_schedule['Arrival_Time_Slot'] == 'Afternoon', 2, 1)\n",
    "\n",
    "sorted_df = assigned_schedule.sort_values(['Day_of_Week', 'Assigned_Tech', 'Arrival_Time_Slot_int','job_number'], ascending=True)\n",
    "\n",
    "#Get the cumulative Job number for a given day and tech\n",
    "assigned_schedule['cumulative_job_number'] = sorted_df.groupby(['Assigned_Tech', 'Day_of_Week'])['job_number'].cumcount()\n",
    "\n",
    "#display(assigned_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ce1fd9-70b9-4381-8543-e143186368bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assigned_schedule_spark_df = spark.createDataFrame(assigned_schedule)\n",
    "\n",
    "joined_df = assigned_schedule_spark_df.join(\n",
    "    customer_table_df,\n",
    "    on='Customer_ID',\n",
    "    how='inner'\n",
    ") \n",
    "\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"dow_int\",\n",
    "    F.when(joined_df.Day_of_Week == 'Monday', 1)\n",
    "     .when(joined_df.Day_of_Week == 'Tuesday', 2)\n",
    "     .when(joined_df.Day_of_Week == 'Wednesday', 3)\n",
    "     .when(joined_df.Day_of_Week == 'Thursday', 4)\n",
    "     .when(joined_df.Day_of_Week == 'Friday', 5)\n",
    "     .otherwise(10)\n",
    ")\n",
    "\n",
    "# Sort by tech, day, and time slot first\n",
    "sorted_data_df = joined_df.orderBy(\n",
    "    ['Assigned_Tech', 'dow_int', 'cumulative_job_number']\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e098b17c-65a0-4476-b99f-107ff349e80d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points on Earth using Haversine formula.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lat1, lon1 : float\n",
    "        Latitude and longitude of first point (in decimal degrees)\n",
    "    lat2, lon2 : float\n",
    "        Latitude and longitude of second point (in decimal degrees)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Distance in miles\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Distance from Raleigh to Durham, NC\n",
    "    >>> dist = haversine_distance(35.7796, -78.6382, 35.9940, -78.8986)\n",
    "    >>> print(f\"{dist:.2f} miles\")\n",
    "    \"\"\"\n",
    "    # Convert to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    # Earth radius in miles (use 6371 for kilometers)\n",
    "    miles = 3956 * c\n",
    "    \n",
    "    return miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4d3f1a-20e4-44b2-98cf-1664186aa28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit, lag\n",
    "from pyspark.sql.window import Window\n",
    "# Register the UDF (User Defined Function) for Spark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35851f76-a04e-44cc-bbd4-025dcba9dcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically add a \"return to home\" row for each technician and day\n",
    "max_job_df = sorted_data_df.groupBy(\"Assigned_Tech\", \"Day_of_Week\") \\\n",
    "    .agg(F.max(\"cumulative_job_number\").alias(\"max_cum_job\"))#sorted_data_df  df_with_distances\n",
    "\n",
    "return_home_df = max_job_df.withColumn(\"cumulative_job_number\", F.col(\"max_cum_job\") + 1) \\\n",
    "    .withColumn(\"Latitude\", lit(STARTING_LAT)) \\\n",
    "    .withColumn(\"Longitude\", lit(STARTING_LON)) \\\n",
    "    .withColumn(\"from_latitude\", lit(None).cast(\"double\")) \\\n",
    "    .withColumn(\"from_longitude\", lit(None).cast(\"double\")) \\\n",
    "        .withColumn('Arrival_Time_Slot', lit('Afternoon'))\n",
    "\n",
    "# Add other columns as null or default as needed to match sorted_data_df schema\n",
    "for col_name in sorted_data_df.columns:\n",
    "    if col_name not in return_home_df.columns:\n",
    "        return_home_df = return_home_df.withColumn(col_name, lit(None))\n",
    "\n",
    "# Select columns in the same order as df_with_distances\n",
    "return_home_df = return_home_df.select(sorted_data_df.columns)\n",
    "\n",
    "# Union the return_home_df to the original dataframe\n",
    "sorted_data_df = sorted_data_df.unionByName(return_home_df)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ee258e-d10e-44e3-b35f-5c1e750b650f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window specification: partition by technician and day, order by job number\n",
    "# Create columns for previous location using lag function\n",
    "window_spec = Window.partitionBy(\"Assigned_Tech\", \"Day_of_Week\") \\\n",
    "                    .orderBy(\"cumulative_job_number\")\n",
    "\n",
    "df_with_prev = sorted_data_df.withColumn(\"prev_latitude\", lag(\"Latitude\").over(window_spec)) \\\n",
    "                 .withColumn(\"prev_longitude\", lag(\"Longitude\").over(window_spec)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32208e05-39ed-4188-ab9a-e0a4c6ee7d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# For the first job of each tech/day, use the starting point\n",
    "# For subsequent jobs, use the previous job's location\n",
    "df_with_distances = df_with_prev.withColumn(\n",
    "    \"from_latitude\",\n",
    "    when(col(\"prev_latitude\").isNull(), lit(STARTING_LAT))\n",
    "    .otherwise(col(\"prev_latitude\"))\n",
    ").withColumn(\n",
    "    \"from_longitude\", \n",
    "    when(col(\"prev_longitude\").isNull(), lit(STARTING_LON))\n",
    "    .otherwise(col(\"prev_longitude\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b2e67ef-af34-4e02-b12c-f625549dc10a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765808935013}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "haversine_udf = udf(haversine_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe7d6d7-a28b-4278-b719-e0fd3f24cb57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_distances = df_with_distances.withColumn(\"Latitude\", col(\"Latitude\").cast(\"double\"))\\\n",
    "    .withColumn(\"Longitude\", col(\"Longitude\").cast(\"double\"))\\\n",
    "        .withColumn('Historic_Avg_Ticket', col('Historic_Avg_Ticket').cast('double'))\\\n",
    "            .withColumn('Historic_Close_Rate', col('Historic_Close_Rate').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a28a02-7340-4761-adb7-abb9df4328ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate distance using the haversine UDF\n",
    "df_final = df_with_distances.withColumn(\n",
    "    \"distance_miles\",\n",
    "    haversine_udf(\n",
    "        col(\"from_latitude\"),\n",
    "        col(\"from_longitude\"),\n",
    "        col(\"Latitude\"),\n",
    "        col(\"Longitude\")\n",
    "    )\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37bec55-9cf5-4fe3-9363-eea8a77993c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get expected Revenue Per Day / Tech, and Per Day / Tech / Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2862817f-049d-44ea-baba-f581e15b4acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate distance using the haversine UDF\n",
    "df_final = df_final.withColumn(\n",
    "    \"expected_rev\", col(\"Historic_Avg_Ticket\") * col(\"Historic_Close_Rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e9156a-b947-4a1c-9fa7-68530f29f2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_dow = df_final.groupBy(\"Day_of_Week\", \"Assigned_Tech\").agg(\n",
    "    F.sum(\"expected_rev\").alias(\"total_expected_rev\"),\n",
    "        F.sum('distance_miles').alias('total_miles')\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd688c3a-54c4-4019-adb8-4b5dbbfdd04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimization Part 1: Without removing techs from jobs, what is the correct order a given job should have been done for that day and tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a41944-f285-4369-96df-e3cd241a34ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_distance_matrix(latitudes, longitudes):\n",
    "    \"\"\"\n",
    "    Create a distance matrix from lists of coordinates.\n",
    "    \n",
    "    This is the CORE FUNCTION you need for TSP!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    latitudes : list or array\n",
    "        List of latitude values for all locations\n",
    "    longitudes : list or array\n",
    "        List of longitude values for all locations\n",
    "        (must be same length as latitudes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        2D array where distances[i][j] = distance from location i to location j\n",
    "        Shape: (n, n) where n = number of locations\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> lats = [35.7796, 35.9940, 36.0999]  # Raleigh, Durham, Chapel Hill\n",
    "    >>> lons = [-78.6382, -78.8986, -79.0654]\n",
    "    >>> distances = create_distance_matrix(lats, lons)\n",
    "    >>> \n",
    "    >>> # Now use with any TSP algorithm!\n",
    "    >>> print(f\"Raleigh to Durham: {distances[0][1]:.2f} miles\")\n",
    "    >>> print(f\"Durham to Chapel Hill: {distances[1][2]:.2f} miles\")\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(latitudes)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if len(longitudes) != n:\n",
    "        raise ValueError(\"latitudes and longitudes must have same length\")\n",
    "    \n",
    "    # Initialize n x n matrix of zeros\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Calculate distance between every pair of locations\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                # Distance from a location to itself is 0\n",
    "                distance_matrix[i][j] = 0\n",
    "            else:\n",
    "                # Calculate actual distance\n",
    "                distance = haversine_distance(\n",
    "                    latitudes[i], longitudes[i],\n",
    "                    latitudes[j], longitudes[j]\n",
    "                )\n",
    "                distance_matrix[i][j] = distance\n",
    "    \n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8bae1f-ab73-41d5-b42d-0e6b1319a800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tsp_nearest_neighbor(distances, start=0):\n",
    "    \"\"\"Fast greedy approach - go to closest unvisited city\"\"\"\n",
    "    n = len(distances)\n",
    "    unvisited = set(range(n))\n",
    "    current = start\n",
    "    route = [current]\n",
    "    unvisited.remove(current)\n",
    "    total_distance = 0\n",
    "    \n",
    "    while unvisited:\n",
    "        # Find nearest unvisited city\n",
    "        nearest = min(unvisited, key=lambda city: distances[current][city])\n",
    "        total_distance += distances[current][nearest]\n",
    "        current = nearest\n",
    "        route.append(current)\n",
    "        unvisited.remove(current)\n",
    "    \n",
    "    # Return to start\n",
    "    total_distance += distances[current][start]\n",
    "    route.append(start)\n",
    "    \n",
    "    return route, total_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b8107a-30f5-4d72-b198-393fe87939df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Optimize the Route of a given tech for a given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18eb6529-2a14-46b3-90dc-0de507cd76ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbc8504-9936-49d5-bbfb-2af93b8bf4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Get necessary columns for TSP\n",
    "data_for_tsp = df_final[['Assigned_Tech','Day_of_Week','Latitude','Longitude','cumulative_job_number']].toPandas()\n",
    "#Adjust dataframe so that the \"final\" job, which is home, is now the starting position. This will be sorted later\n",
    "data_for_tsp.loc[data_for_tsp['cumulative_job_number'] == 7, 'cumulative_job_number'] = 0\n",
    "\n",
    "#data_for_tsp = data_for_tsp[(data_for_tsp['Assigned_Tech']=='Pat') & (data_for_tsp['Day_of_Week']=='Monday')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c342b46e-2076-4dad-9fbe-3bb5b3208fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_distance_matrix(coordinates):\n",
    "    \"\"\"\n",
    "    Create a distance matrix from an array of (latitude, longitude) tuples.\n",
    "    \n",
    "    Args:\n",
    "        coordinates: List of (lat, lon) tuples\n",
    "        \n",
    "    Returns:\n",
    "        2D numpy array with distances between all points\n",
    "    \"\"\"\n",
    "    n = len(coordinates)\n",
    "    matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                matrix[i][j] = haversine_distance(\n",
    "                    coordinates[i][0], coordinates[i][1],\n",
    "                    coordinates[j][0], coordinates[j][1]\n",
    "                )\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e23856c-f3ea-4e46-a92c-214984c39d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def optimize_routes(df):\n",
    "    \"\"\"\n",
    "    Process technician schedule data and optimize routes using TSP nearest neighbor.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe columns: \n",
    "                   Assigned_Tech, Day_of_Week, from_latitude, from_longitude, cumulative_job_number\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with optimized routes for each technician and day\n",
    "    \"\"\" \n",
    "    \n",
    "    # Dictionary to store all optimized routes\n",
    "    optimized_routes = {}\n",
    "    \n",
    "    # Group by technician and day\n",
    "    for (tech, day), group in df.groupby(['Assigned_Tech', 'Day_of_Week']):\n",
    "        # Sort by cumulative_job_number to maintain original order for reference\n",
    "        group = group.sort_values('cumulative_job_number')\n",
    "        \n",
    "        # Extract coordinates\n",
    "        coordinates = list(zip(group['Latitude'], group['Longitude']))\n",
    "        \n",
    "        # Create distance matrix\n",
    "        distance_matrix = create_distance_matrix(coordinates)\n",
    "        \n",
    "        # Find optimal route using TSP nearest neighbor\n",
    "        # Start at index 0 (first job of the day)\n",
    "        optimal_route_indices, total_distance = tsp_nearest_neighbor(distance_matrix, start=0)\n",
    "        \n",
    "        # Map indices back to original job information\n",
    "        jobs_list = group.reset_index(drop=True)\n",
    "        optimal_route = []\n",
    "        \n",
    "        for idx in optimal_route_indices:\n",
    "            job_info = {\n",
    "                'optimized_job_number': idx,\n",
    "                'original_job_number': int(jobs_list.iloc[idx]['cumulative_job_number']),\n",
    "                'latitude': jobs_list.iloc[idx]['Latitude'],\n",
    "                'longitude': jobs_list.iloc[idx]['Longitude']\n",
    "            }\n",
    "            optimal_route.append(job_info)\n",
    "        \n",
    "        # Store in results\n",
    "        key = f\"{tech}_{day}\"\n",
    "        optimized_routes[key] = {\n",
    "            'technician': tech,\n",
    "            'day': day,\n",
    "            'total_jobs': len(coordinates),\n",
    "            'total_optimized_distance_mi': round(total_distance, 2),\n",
    "            'route': optimal_route\n",
    "        }\n",
    "    \n",
    "    return optimized_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6de5f24-4791-495a-ab2b-7bc3b7967d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_data = optimize_routes(data_for_tsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80def7a-f538-433d-99b0-8550d74829af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "optimized_route = pd.DataFrame([\n",
    "    {\n",
    "        'technician': v['technician'],\n",
    "        'day': v['day'],\n",
    "        'total_jobs': v['total_jobs'],\n",
    "        'total_optimized_distance_mi': v.get('total_optimized_distance_mi', None),\n",
    "        'route': v['route']\n",
    "    }\n",
    "    for v in optimized_data.values()\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658868ec-1ef5-4aa4-a92d-13a43d482b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_explode = optimized_route.explode('route').reset_index(drop=True)\n",
    "df_explode = pd.concat([\n",
    "    df_explode.drop(['route'], axis=1),\n",
    "    df_explode['route'].apply(pd.Series)\n",
    "], axis=1)\n",
    " \n",
    "\n",
    "df_explode['new_job_order'] = (\n",
    "    df_explode.groupby(['technician', 'day'])\n",
    "    .cumcount()\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fae2eb1-fd3c-4634-ad02-c6db5fef6eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_df = df_explode[['day','technician','total_optimized_distance_mi']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf424670-637b-4a07-bb7e-588a7ba79793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert df (Pandas DataFrame) to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(optimized_df)\n",
    "\n",
    "# Join df_spark into results_dow on Day_of_Week and Assigned_Tech\n",
    "joined_df = result_dow.join(\n",
    "    df_spark,\n",
    "    (result_dow['Day_of_Week'] == df_spark['day']) & \n",
    "    (result_dow['Assigned_Tech'] == df_spark['technician']),\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0099903c-a9ea-4114-87b6-e611d47588bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_route_df = joined_df[['Day_of_Week','Assigned_Tech','total_expected_rev','total_miles','total_optimized_distance_mi']]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Proximity Problem Counter Factual",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}